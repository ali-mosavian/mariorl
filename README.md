# Mario RL

**Reinforcement Learning for Super Mario Bros with World Models**

A modular deep reinforcement learning framework featuring world models with latent representations, distributed training, and advanced DQN techniques.

## âœ¨ Features

- ğŸ§  **World Model Architecture** - Learn abstract latent representations for better generalization
- ğŸš€ **Distributed Training** - Multi-worker parallel data collection
- ğŸ¯ **Dueling Double DQN** - Advanced Q-learning with target networks
- ğŸ”„ **Prioritized Experience Replay** - Sample important transitions more frequently
- ğŸ“Š **Comprehensive Metrics** - Track reconstruction quality, Q-values, and training progress
- ğŸ³ **Docker Support** - Ready for deployment on RunPod and other cloud services

## ğŸ® World Model Overview

The world model learns to:
1. **Encode** raw pixel frames into compact latent representations (z)
2. **Predict** next latent states given current state and action (dynamics model)
3. **Estimate** rewards from latent states
4. **Decode** latent states back to frames (for validation)

The Q-network then operates entirely in latent space, enabling:
- Faster training
- Better generalization across levels
- More abstract reasoning

## ğŸ“¦ Installation

### Quick Start (with uv)

```bash
# Clone repository
git clone https://github.com/yourusername/mario-rl.git
cd mario-rl

# Install with uv (recommended)
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync

# Or install with pip
pip install -e .
```

### For Development

```bash
# Install with dev dependencies
uv sync --extra dev

# Or with pip
pip install -e ".[dev]"
```

## ğŸš€ Quick Start

### Training

```bash
# Basic training on level 1-1
uv run python scripts/train.py --level 1,1 --workers 4 --world-model

# Train for longer with more workers
uv run python scripts/train.py \
  --level 1,1 \
  --workers 8 \
  --learner-steps 100000 \
  --world-model \
  --wm-steps 500 \
  --q-steps 500

# Train on different level
uv run python scripts/train.py --level 2,1 --workers 6 --world-model
```

### Watch Trained Agent

```bash
# Watch the agent play
uv run python scripts/watch.py checkpoints/<run-name>/weights.pt --world-model

# Watch on a different level
uv run python scripts/watch.py checkpoints/<run-name>/weights.pt --world-model --level 1-2
```

## ğŸ“‚ Project Structure

```
mario-rl/
â”œâ”€â”€ mario_rl/                    # Main package
â”‚   â”œâ”€â”€ agent/                   # Neural networks and replay buffers
â”‚   â”‚   â”œâ”€â”€ world_model.py       # World model architecture
â”‚   â”‚   â”œâ”€â”€ neural.py            # DuelingDDQN network
â”‚   â”‚   â””â”€â”€ replay.py            # Experience replay buffer
â”‚   â”œâ”€â”€ environment/             # Mario environment and wrappers
â”‚   â”‚   â”œâ”€â”€ mariogym.py          # Multi-level Mario gym
â”‚   â”‚   â””â”€â”€ wrappers.py          # Frame skip, resize, etc.
â”‚   â”œâ”€â”€ training/                # Distributed training system
â”‚   â”‚   â”œâ”€â”€ world_model_learner.py  # World model + Q-network training
â”‚   â”‚   â”œâ”€â”€ learner.py           # Standard DQN learner
â”‚   â”‚   â”œâ”€â”€ worker.py            # Experience collection worker
â”‚   â”‚   â”œâ”€â”€ shared_buffer.py     # Multiprocess replay buffer
â”‚   â”‚   â””â”€â”€ training_ui.py       # Curses-based training UI
â”‚   â””â”€â”€ utils/                   # Utilities and metrics
â”œâ”€â”€ scripts/                     # Command-line scripts
â”‚   â”œâ”€â”€ train.py                 # Main training script
â”‚   â”œâ”€â”€ watch.py                 # Watch agent play
â”‚   â””â”€â”€ train_runpod.sh          # RunPod training script
â”œâ”€â”€ docker/                      # Docker configuration
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .dockerignore
â”œâ”€â”€ docs/                        # Documentation
â”‚   â””â”€â”€ RUNPOD.md                # RunPod deployment guide
â””â”€â”€ tests/                       # Unit tests

```

## ğŸ”§ Configuration

### Training Options

| Option | Default | Description |
|--------|---------|-------------|
| `--level` | `1,1` | Mario level (world,stage) |
| `--workers` | `4` | Number of worker processes |
| `--learner-steps` | `-1` | Max training steps (infinite if -1) |
| `--buffer-size` | `50000` | Replay buffer capacity |
| `--batch-size` | `64` | Training batch size |
| `--world-model` | `False` | Use world model architecture |
| `--latent-dim` | `128` | Latent space dimension |
| `--wm-steps` | `500` | World model steps per cycle |
| `--q-steps` | `500` | Q-network steps per cycle |
| `--ui` / `--no-ui` | `True` | Show training UI |

### World Model Training

The training alternates between two phases:

1. **World Model Phase** (500 steps):
   - Train encoder/decoder on reconstruction
   - Train dynamics model to predict next latent state
   - Train reward predictor

2. **Q-Network Phase** (500 steps):
   - Freeze world model
   - Train Q-network in latent space
   - Update target network periodically

## ğŸ“Š Monitoring Training

### Interactive UI

By default, training shows a curses-based UI with:
- Worker statistics (episodes, rewards, x-position)
- Learner metrics (loss, Q-values, buffer size)
- World model metrics (MSE, SSIM, dynamics loss)

### Log Files

All runs save to `checkpoints/<timestamp>/`:
- `weights.pt` - Latest network weights
- `checkpoint.pt` - Full training state (for resumption)
- `training.csv` - Metrics logged every 100 steps
- `training.log` - Full training log

### Plot Metrics

```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('checkpoints/<run>/training.csv')

fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes[0,0].plot(df['step'], df['wm_recon_mse'])
axes[0,0].set_title('Reconstruction MSE')

axes[0,1].plot(df['step'], df['wm_ssim'])
axes[0,1].set_title('SSIM')

axes[1,0].plot(df['step'], df['q_mean'])
axes[1,0].set_title('Q Mean')

axes[1,1].plot(df['step'], df['q_loss'])
axes[1,1].set_title('Q Loss')

plt.tight_layout()
plt.show()
```

## ğŸ³ Docker & RunPod

### Build Docker Image

```bash
cd docker
docker build -t mario-rl .
```

### Run Locally with Docker

```bash
docker run --gpus all \
  -v $(pwd)/checkpoints:/workspace/checkpoints \
  -e WORKERS=8 \
  -e STEPS=100000 \
  mario-rl
```

### Deploy on RunPod

See [docs/RUNPOD.md](docs/RUNPOD.md) for detailed instructions.

Quick start:
```bash
# On RunPod instance
git clone <your-repo> /workspace/mario-rl
cd /workspace/mario-rl
./scripts/train_runpod.sh
```

## ğŸ§ª Testing

```bash
# Run all tests
uv run pytest

# Run with coverage
uv run pytest --cov=mario_rl --cov-report=html

# Run specific test
uv run pytest tests/test_world_model.py
```

## ğŸ“ˆ Performance

### Expected Training Time (RTX 4090)

- **10k steps**: ~30 min (early learning)
- **50k steps**: ~2.5 hours (decent behavior)
- **100k steps**: ~5 hours (good performance)
- **200k steps**: ~10 hours (strong agent)

### Signs of Good Training

- **Reconstruction MSE** drops to <0.01
- **SSIM** increases to >0.9
- **Q-values** become positive
- **Worker x-positions** steadily increase
- **Episode rewards** improve over time

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) for details

## ğŸ™ Acknowledgments

- Based on [Dueling DQN](https://arxiv.org/abs/1511.06581)
- Inspired by [World Models](https://worldmodels.github.io/)
- Built with [gym-super-mario-bros](https://github.com/Kautenja/gym-super-mario-bros)

## ğŸ“š Citation

If you use this code in your research, please cite:

```bibtex
@software{mario_rl_2025,
  author = {Your Name},
  title = {Mario RL: Reinforcement Learning for Super Mario Bros with World Models},
  year = {2025},
  url = {https://github.com/yourusername/mario-rl}
}
```

## ğŸ“§ Contact

- Issues: [GitHub Issues](https://github.com/yourusername/mario-rl/issues)
- Email: your.email@example.com

